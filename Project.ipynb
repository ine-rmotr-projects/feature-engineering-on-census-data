{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "## Feature Engineering on Census Data\n",
    "\n",
    "In this project, you will be working with Census Data from 1994 to put in practice all the techniques you learned on previous lessons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## What we know about the data\n",
    "\n",
    "The 1994 Census Income dataset has **48,842 entries** (split into train and test). Each entry contains the following information about an individual:\n",
    "- **age**: the age of an individual\n",
    "- **workclass**: a general term to represent the employment status of an individual\n",
    "- **fnlwgt**: final weight. In other words, this is the number of people the census believes the entry represents.\n",
    "- **education**: the highest level of education achieved by an individual.\n",
    "- **education_num**: the highest level of education achieved in numerical form.\n",
    "- **marital_status**: marital status of an individual. Married-civ-spouse corresponds to a civilian spouse while Married-AF-spouse is a spouse in the Armed Forces.\n",
    "- **occupation**: the general type of occupation of an individual\n",
    "- **relationship**: represents what this individual is relative to others. For example an individual could be a Husband. Each entry only has one relationship attribute.\n",
    "- **race**: Descriptions of an individual’s race\n",
    "- **sex**: the biological sex of the individual\n",
    "- **capital_gain**: capital gains for an individual\n",
    "- **capital_loss**: capital loss for an individual\n",
    "- **hours_per_week**: the hours an individual has reported to work per week\n",
    "- **native_country**: country of origin for an individual\n",
    "- **income_bracket**: whether or not an individual makes more than $50,000 annually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and alias as pd\n",
    "\n",
    "# Use this to view all of your data\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the train set datasets/wage_train.csv\n",
    "\n",
    "# Look at the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the test set, apply the same processes as train, then predict\n",
    "\n",
    "# Look at the first few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Remember that:\n",
    "\n",
    "- We **develop from train**.  \n",
    "- We **apply to test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step in exploration\n",
    "# Use .describe to look at the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## What we get from the data description\n",
    "\n",
    "- It seems like fnlwgt isn't really a characteristic of the individuals\n",
    "- Also, since the Census Bureau assigns that value, we won't likely have it when making predictions on unseen data.\n",
    "\n",
    "> Let's remove that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the fnlwgt feature from train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### List all binary columns and indicate if the classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### List all nominal columns and indicate the majority class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### List all ordinal columns and indicate the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: occupation could have somewhat of an ordinal quality to it, but it's not clear how to order some of the occupations.  It also seems as if the ordinal quality may be more related to either work/life balance or income -> or some combination of the two.  In absence of a good way to rank occupations, we will treat them as nominal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### List any cyclical or date columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### List all continuous columns with basic stats (mean, std, min and max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## What else can we get from the description?\n",
    "\n",
    "- That 'education_num' is the numeric representation of 'education'\n",
    "- We can drop 'education' since it is ordinal and already numerically represented by 'education_num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop education from train and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Final piece of information we get from the description\n",
    "\n",
    "There are a few serious majority categories:\n",
    "\n",
    "   - United States has very nearly ALL of the native_country examples.  This is a serious majority category. (native_country feature) \n",
    "   - White is the same. (race feature)\n",
    "   - Private is the same. (workclass feature)\n",
    "   - Married-civ-spouse isnt' as big, but we'll remove it too.  It can help with collinearity.\n",
    "   \n",
    "When we get dummies for this variable, instead of drop_first we'll want to drop United States and White specifically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Missing data\n",
    "\n",
    "We don't see any nulls using .describe().  Are there any **hidden nulls** in this data?\n",
    "What kind of features would we check for hidden nulls?\n",
    "\n",
    "This will require a little manual work.\n",
    "> Use .value_counts() to check for hidden nulls.  I've done the first one for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for hidden nulls in workclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a hidden null in workclass.  \n",
    "# Let's replace it with a true null and figure out what to do with it later.\n",
    "df_train.loc[df_train.workclass=='?','workclass'] = None\n",
    "# Check results using value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hmmmm.  That didn't work.\n",
    "\n",
    "Why isn't that working?  \n",
    "\n",
    "> Let's have a different look at the values with .unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the values in workclass with the unique() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## There are some extra spaces in those strings.\n",
    "\n",
    "> Let's clean that up -> it will make life easier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all of the extra white space from object (string value) columns using the string method .strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the same to test\n",
    "\n",
    "\n",
    "# Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Now that that's all clean:\n",
    "> Let's try to fix the nulls again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace the ? in workclass with a true null\n",
    "\n",
    "# Now do the same to test\n",
    "\n",
    "# Check results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## What else do we get from our .describe() above that can help us here?\n",
    "\n",
    "- We see that there are no negative values.  We don't need to check for -1 representation of nulls.\n",
    "\n",
    "> Let's check the remaining nominal features for hidden nulls and fix them. (Create as many new cells as you need to do this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of nominal features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each of the nominal features for hidden nulls using .value_counts()\n",
    "# If you find hidden nulls, convert them to true nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did you check every nominal feature?\n",
    "\n",
    "#### Did you apply any necessary changes to both train and test data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## What do we do with the null values?\n",
    "\n",
    "> Let's see how much data we lose if we remove all rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try removing all rows with nulls (you probably want to save the result to a new object)\n",
    "\n",
    "\n",
    "# How much of the data was removed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## If removing all null rows removes less than 20% of the data:\n",
    "\n",
    "Go ahead and remove all null rows for now. We can come back in a future iteration and spend more time here if model performance isn't as high as we would like for it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with null values in train and test\n",
    "\n",
    "\n",
    "# Check the new length of your train and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Convert string values to numeric\n",
    "\n",
    "**Remember**: all of our values have to be numeric for a machine learning algorithm to accept and understand them.\n",
    "> Let's convert all of our string values to numeric now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Nominal to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change all of our nominal features to numeric format\n",
    "# Use the shortcut method get_dummies to get dummy columns for train data\n",
    "# Remember we want to pick the category to drop with race and native_country\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drop the White dummy column and United States dummy col\n",
    "\n",
    "# We can get a full list of columns to match to test using .columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply this to our test set.  \n",
    "# Do NOT drop_first here.  We'll use our train column list to match up the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Did you get an error?\n",
    "\n",
    "I did. Looks like we have a feature in train that isn't in test.\n",
    "\n",
    "We'll need to add it to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Holand-Netherlands feature exists in train but not in test.  We'll need to add that column to our test data\n",
    "# It's a binary column.  We'll fill it with all 0s\n",
    "# Make sure the name matches exactly what you see in the KeyError\n",
    "\n",
    "# Now match up the train and test cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the columns in train dummies to test dummies\n",
    "for train_col,test_col in zip(df_train_dummies.columns,df_test_dummies.columns):\n",
    "    print(train_col,'<---->',test_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## If your columns match-up, move on!  If not, you better find the bug now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Binary to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of binary features\n",
    "\n",
    "# Create mappings for binary cols\n",
    "# Keep information needed to apply this to test data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train binary to numeric\n",
    "# Copy your train data into a new data frame 'df_train_binary' (just trust me on this one)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results with .head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's appply the mappings to our test set\n",
    "# Don't worry if these results look funny.  Try to apply the mappings and then we'll figure out\n",
    "# what is going on here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Do your results on test look wrong?\n",
    "\n",
    "That's ok, mine do too.\n",
    "\n",
    "> Let's figure out why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why didn't our mapping work on income_bracket?\n",
    "# Let's have a look at our test data before dealing with binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did our train feature look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test values have '.' at the end.  \n",
    "# Let's clean this up and try again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the mappings to our test set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Phew!  Glad we cleared that up!\n",
    "\n",
    "You'll spend the majority of your time cleaning up your data like this.  It's not fun, but it must be done.  I've never worked with any company that has perfectly clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Pulling all of the data together\n",
    "\n",
    "Now that we have our dummies, binary mappings and original continuous data, we need to put it together.\n",
    "\n",
    "> Let's use pd.concat() to pull all of the pieces of data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If df_train isn't already in one dataframe, join the pieces now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test, you need to pull the dummies df and the binary df together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The length of your test sets should all match each other as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the length of your train data is not the same across all of your train sets, something is wrong.  Go back and look for a bug.\n",
    "\n",
    "#### The length of your test sets should all match each other as well.\n",
    "\n",
    "#### Test length should be a small percent of your train length.  These should not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the relevant columns\n",
    "# List all columns we want to keep from the df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for the train data . . .\n",
    "# concat together the dummies, the binary columns and the relevant continuous columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same for test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do another check to make sure your train columns match your test columns\n",
    "for train_col,test_col in zip(df_train_all.columns,df_test_all.columns):\n",
    "    print(train_col,'<---->',test_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Collinearity\n",
    "\n",
    "**Remember**: we want our features to be correlated with the target -> but not with each other\n",
    "\n",
    "Now that we've taken a peak at correlation, skew and outliers:\n",
    "> Let's check for collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use statsmodels variance_inflation_factor to check for collinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check train features for collinearity\n",
    "# Get a list of feature columns\n",
    "\n",
    "# Separate features from the target into X_train_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the VIF function to calculate VIF on X_train_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Remember! VIF greater than 10 is an issue.\n",
    "\n",
    "For now remove most columns with VIF greater than 10.  In a future iteration, you can derive or engineer some features with colinear columns and try to improve model performance.\n",
    "\n",
    "You can leave a few and re-run VIF to see if it eleviates collinearity.\n",
    "**Hint**: You could use a correlation score with the collinear features and the target to determine which are the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of collinear columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check collinearity of these features with the target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to think about the removal\n",
    "\n",
    "- age and education probably have some relation.  Let's drop age since it's a little less correlated with the target.\n",
    "- many of the marital-status categories are some version of not married - let's combine them into one group called not_married"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature called nom_dum_Not-married that combines:\n",
    "# nom_dum_Unmarried, nom_dum_Divorced and nom_dum_Never-married\n",
    "\n",
    "# Figure out what percent of the new feature is marked 1 (indicating not mraried)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of columns to remove for collinearity based on the above\n",
    "\n",
    "# Drop those columns\n",
    "\n",
    "# Run VIF again\n",
    "\n",
    "# This is an iterative process so you might have to do this several times to refine your cols_to_remove list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### education_num and hours_per_week are still collinear.  \n",
    "education_num has the highest correlation with the target.  We'll drop hours_per_week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drom hours_per_week from your features\n",
    "\n",
    "# Run VIF again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your list of columns to remove to include hours_per_week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the derived column (nom_dum_Not-married) in train and test\n",
    "\n",
    "\n",
    "# When you have a final list, drop columns from your train and test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Outliers, Skew, Transform, Scale\n",
    "\n",
    "Once that lines up, let's check the correlation of the features to the target.\n",
    "\n",
    "The target in this data is income_bracket.\n",
    "\n",
    "> Let's check the correlation of the features with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation of all of the columns with the target\n",
    "# Because there are a lot of columns, it might help to order them using .sort_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skew in feature columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### There are too many columns to really consider all at once\n",
    "\n",
    "Many of these skewed columns are imbalanced binary columns (from dummy encoding)\n",
    "\n",
    "> Let's just check correlation and skew on our continuous columns (you can iterate on this later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of non_binary columns\n",
    "\n",
    "\n",
    "# Check skew on the non_binary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation of the non-binary columns and the target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each non_binary column using .hist() and sns.boxplot() \n",
    "# When you look at the results, consider whether the data is skewed or normally distributed.\n",
    "\n",
    "# Visualize capital_gain with .hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Visualize the data in a boxplot to get a better understanding of outliers and skew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the rest of the non-binary columns with .hist() and sns.boxplot()\n",
    "# Create as many new cells as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## How to deal with skew\n",
    "Should you remove outliers in this data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### If the data is skewed, transform it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import power transformer from sklearn\n",
    "\n",
    "\n",
    "# Instantiate a PT instances\n",
    "\n",
    "\n",
    "# Split features from the target in both train and test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit on train data and transform train data\n",
    "\n",
    "\n",
    "# Transform test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### We have a LOT of columns!  Let's look at the shape of our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the shape method to get the number of columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "## Let's reduce the dimensionality using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to scale data prior to PCA\n",
    "# Use StandardScaler to scale the features\n",
    "\n",
    "\n",
    "# Instantiate a StandardScaler instance\n",
    "\n",
    "# Fit on train and then transform train\n",
    "\n",
    "# Transform Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the features\n",
    "# Import PCA from sklearn\n",
    "\n",
    "\n",
    "# Instantiate a PCA instance\n",
    "# Typically, we want the explained variance to be between 95–99%\n",
    "\n",
    "# Put the results in a dataframe format that's easier to read\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many columns did we have prior to PCA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many columns do we have after PCA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the test set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that test has the same number of columns as train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target variable is binary.  Let's try to run this through simple Logistic Regression model\n",
    "# Remember, we only pulled in our training data.  When we are ready to test, we will need to apply\n",
    "# the same changes on our test set that we applied to our train.\n",
    "\n",
    "# Import LogisticRegression from sklearn\n",
    "\n",
    "# Instantiate a logistic regression instance\n",
    "\n",
    "# Fit the model on train data\n",
    "\n",
    "# Make predictions from the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "# Import evaluation tools from sklearn\n",
    "\n",
    "# Evaluate performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation will be covered in another class\n",
    "# But we'll get a quick baseline (the accuracy if we predicted everything as the majority class)\n",
    "baseline = 1 - (y_test.sum()/len(y_test))\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [EXTRA] Can you improve on this score by iterating back through the feature engineering and continuing to the process?\n",
    "\n",
    "Potential areas for improvement:\n",
    "\n",
    "- Impute missing values\n",
    "- Think more about how you remove data generally: e.g., when removing columns for collinearity during dummy encoding and collinearity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98729912-57be3e80-237a-11eb-80e4-233ac344b391.png\"></img>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
